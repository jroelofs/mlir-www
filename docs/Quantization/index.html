<!doctype html><html lang=en-us><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=no"><title>MLIR Quantization - MLIR</title><meta name=description content="Multi-Level IR Compiler Framework"><meta name=generator content="Hugo 0.64.1"><link href=https://mlir.llvm.org/index.xml rel=alternate type=application/rss+xml><link rel=canonical href=https://mlir.llvm.org/docs/Quantization/><link rel=stylesheet href=https://mlir.llvm.org/css/theme.css><script src=https://use.fontawesome.com/releases/v5.0.6/js/all.js></script><link rel=stylesheet href=https://mlir.llvm.org/css/chroma.min.css><script src=https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js></script><script src=https://cdn.jsdelivr.net/npm/jquery.easing@1.4.1/jquery.easing.min.js></script><script src=https://mlir.llvm.org/js/bundle.js></script><style>:root{}</style></head><body><div class=container><header><h1><div><img src=https://mlir.llvm.org//mlir-logo.png width=40px align=absmiddle>
MLIR</div></h1><p class=description>Multi-Level IR Compiler Framework</p></header><div class=global-menu><nav><ul><li class=parent><a href>Community<i class="fas fa-angle-right"></i></a><ul class=sub-menu><li class=child><a href=https://llvm.discourse.group/c/mlir/31>Forums</a></li><li class=child><a href=https://discord.gg/xS7Z362>Chat</a></li></ul></li><li><a href=/getting_started/Debugging/>Debugging</a></li><li><a href=/getting_started/Faq/>FAQ</a></li><li class=parent><a href=https://github.com/llvm/llvm-project/tree/master/mlir>Source<i class="fas fa-angle-right"></i></a><ul class=sub-menu><li class=child><a href=doxygen/>Doxygen</a></li><li class=child><a href=https://github.com/llvm/llvm-project/tree/master/mlir>GitHub</a></li></ul></li><li><a href="https://bugs.llvm.org/buglist.cgi?bug_status=__open__&list_id=177877&order=changeddate%20DESC%2Cpriority%2Cbug_severity&product=MLIR&query_format=specific">Bugs</a></li></ul></nav></div><div class=content-container><main><h1>MLIR Quantization</h1><p>This document outlines the design of the MLIR quantization system. While the
term &ldquo;quantization&rdquo; is highly overloaded, in this case, it refers to a fairly
narrow scope of techniques in use to enable conversion of floating-point
computations to corresponding and plausible variants expressed in integer math
for inference, as has historically been supported by low-bit depth inference
engines such as TFLite, various accelerator hardware, and many DSPs.</p><p>Much of this is inspired by the approach taken
<a href=https://arxiv.org/abs/1712.05877>in this paper</a>
with many extensions and
adaptations folded in. It specifically documents the positions that MLIR has
taken on the topic, and is not a general reference.</p><p><nav id=TableOfContents><ul><li><a href=#uniform-quantization>Uniform quantization</a><ul><li><a href=#fixed-point-values>Fixed point values</a></li><li><a href=#affine-values>Affine values</a></li><li><a href=#relation>Relation</a></li><li><a href=#converting-between-real-and-fixed-point-or-affine>Converting between real and fixed point or affine</a></li></ul></li><li><a href=#usage-within-mlir>Usage within MLIR</a></li><li><a href=#quantization-dialect>Quantization Dialect</a><ul><li><a href=#quantized-type>Quantized type</a></li><li><a href=#quantized-type-conversion-operations>Quantized type conversion operations</a></li><li><a href=#instrumentation-and-constraint-operations>Instrumentation and constraint operations</a></li></ul></li><li><a href=#integration-with-simulated-quantization-at-training-time>Integration with simulated quantization at training time</a></li><li><a href=#tflite-native-quantization>TFLite native quantization</a><ul><li><a href=#general-algorithm>General algorithm</a></li></ul></li><li><a href=#fxpmath-dialect>FxpMath dialect</a><ul><li><a href=#real-math-operations>Real math operations</a></li><li><a href=#fixed-point-math-operationss>Fixed-point math operationss</a></li></ul></li><li><a href=#solver-tools>Solver tools</a></li></ul></nav><h2 id=uniform-quantization>Uniform quantization</h2><p>The primary quantization mechanism supported by MLIR is a scheme which can
express fixed point and affine transformations via uniformly spaced point on the
<a href=https://en.wikipedia.org/wiki/Real_number>Real</a>
number line.</p><p>Further, the scheme can be applied:</p><ul><li><em>per-layer</em> : Applying to every value within the target type.</li><li><em>per-axis</em> (also called <em>per-channel</em>) : Applying individually to each index
along a specific axis of a tensor type.</li></ul><h3 id=fixed-point-values>Fixed point values</h3><p><a href=https://en.wikipedia.org/wiki/Fixed-point_arithmetic>Fixed point</a>
values are a
<a href=https://en.wikipedia.org/wiki/Real_number>Real</a>
number divided by a <em>scale</em>.
We will call the result of the divided real the <em>scaled value</em>.</p><p>$$ real_value = scaled_value * scale $$</p><p>The scale can be interpreted as the distance, in real units, between neighboring
scaled values. For example, if the scale is $$ \pi $$, then fixed point values
with this scale can only represent multiples of $$ \pi $$, and nothing in
between. The maximum rounding error to convert an arbitrary Real to a fixed
point value with a given $$ scale $$ is $$ \frac{scale}{2} $$. Continuing the
previous example, when $$ scale = \pi $$, the maximum rounding error will be $$
\frac{\pi}{2} $$.</p><p>Multiplication can be performed on scaled values with different scales, using
the same algorithm as multiplication of real values (note that product scaled
value has $$ scale_{product} = scale_{left \mbox{ } operand} * scale_{right
\mbox{ } operand} $$). Addition can be performed on scaled values, so long as
they have the same scale, using the same algorithm for addition of real values.
This makes it convenient to represent scaled values on a computer as signed
integers, and perform arithmetic on those signed integers, because the results
will be correct scaled values.</p><h3 id=affine-values>Affine values</h3><p>Mathematically speaking, affine values are the result of
<a href=https://en.wikipedia.org/wiki/Affine_transformation#Representation>adding a Real-valued <em>zero point</em>, to a scaled value</a>
.
Alternatively (and equivalently), subtracting a zero point from an affine value results in a
scaled value:</p><p>$$ real_value = scaled_value * scale = (affine_value - zero_point) * scale $$</p><p>Essentially, affine values are a shift of the scaled values by some constant
amount. Arithmetic (i.e., addition, subtraction, multiplication, division)
cannot, in general, be directly performed on affine values; they must first be
<a href=#affine-to-fixed-point>converted</a>
to the equivalent scaled values.</p><p>As alluded to above, the motivation for using affine values is to more
efficiently represent real values that will actually be encountered during
computation. Frequently, real values that will be encountered are not
symmetric around the real zero. We also make the assumption that the real zero
is encountered during computation, and should thus be represented.</p><p>In this case, it is inefficient to store scaled values represented by signed
integers, as some of the signed integers will never be used. In effect, the bit patterns
corresponding to those signed integers are going to waste.</p><p>In order to exactly represent the real zero with an integral-valued affine
value, the zero point must be an integer between the minimum and maximum affine
value (inclusive). For example, given an affine value represented by an 8 bit
unsigned integer, we have: $$ 0 \leq zero_point \leq 255$$. This is important,
because in convolution-like operations of deep neural networks, we frequently
need to zero-pad inputs and outputs, so zero must be exactly representable, or
the result will be biased.</p><h3 id=relation>Relation</h3><p>Real values, fixed point values, and affine values relate through the following
equation, which demonstrates how to convert one type of number to another:</p><p>$$ real_value = scaled_value * scale = (affine_value - zero_point) * scale $$</p><p>Note that computers generally store mathematical values using a finite number of
bits. Thus, while the above conversions are exact, to store the result in a
finite number of bits, we must, in general, round the result of the conversion
(this applies to both cases: storing using floating point and storing using
fixed point). Note that a full discussion of rounding behavior is outside the
scope of this document, and it is safe to assume unless otherwise stated that
rounding should be according to the IEEE754 default of RNE (where hardware
permits).</p><h3 id=converting-between-real-and-fixed-point-or-affine>Converting between real and fixed point or affine</h3><p>To convert a real value to a fixed point value, we must know the scale. To
convert a real value to an affine value, we must know the scale and the zero point.</p><h4 id=real-to-affine>Real to affine</h4><p>To convert an input tensor of real-valued elements (usually represented by a
floating point format, frequently
<a href=https://en.wikipedia.org/wiki/Single-precision_floating-point_format>Single precision</a>
)
to a tensor of affine elements represented by an integral type (e.g. 8-bit
unsigned integer), the following conversion can be performed (note that it is
not required that all representable values of the integral type are used):</p><p>$$
\begin{align*}
af&fine_value_{uint8 , or , uint16} \<br>&= clampToTargetSize(roundToNearestInteger( \frac{real_value_{Single}}{scale_{Single}})_{sint32} + zero_point_{uint8 , or , uint16})
\end{align*}
$$</p><p>In the above, we assume that $$real_value$$ is a Single, $$scale$$ is a Single,
$$roundToNearestInteger$$ returns a signed 32-bit integer, and $$zero_point$$
is an unsigned 8-bit or 16-bit integer. Note that bit depth and number of fixed
point values are indicative of common types on typical hardware but is not
constrained to particular bit depths or a requirement that the entire range of
an N-bit integer is used.</p><h4 id=affine-to-real>Affine to real</h4><p>To convert an output tensor of affine elements represented by uint8
or uint16 to a tensor of real-valued elements (usually represented with a
floating point format, frequently Single precision), the following conversion
can be performed:</p><p>$$
\begin{align*}
re&al_value_{Single} \<br>&= roundToNearestFloat((affine_value_{uint8 , or , uint16} - zero_point_{uint8 , or , uint16})_{sint32})_{Single} * scale_{Single}
\end{align*}
$$</p><p>In the above, we assume that the result of subtraction is in 32-bit signed
integer format, and that $$roundToNearestFloat$$ returns a Single.</p><h4 id=affine-to-fixed-point>Affine to fixed point</h4><p>When the affine and fixed point scales are the same, subtract the zero point
from the affine value to get the equivalent fixed point value.</p><p>$$
scaled_value = affine_value_{non\mbox{-}negative} - zero_point_{non\mbox{-}negative}
$$</p><h4 id=fixed-point-to-affine>Fixed point to affine</h4><p>When the affine and fixed point scales are the same, add the zero point to the
fixed point value to get the equivalent affine value.</p><p>$$
affine_value_{non\mbox{-}negative} = scaled_value + zero_point_{non\mbox{-}negative}
$$</p><h2 id=usage-within-mlir>Usage within MLIR</h2><p>There are several components to the quantization system being developed within
MLIR:</p><ul><li><p><em>Quantization</em> dialect containing:</p><ul><li>A family of
<a href=#quantized-type>QuantizedTypes</a>
which represent the
mapping between <em>expressed</em> values (typically of a floating point
computer type) and <em>storage</em> values (typically of an integral computer
type).</li><li><a href=#quantized-type-conversion-ops>Type conversion ops</a>
for converting
between types based on a QuantizedType and its <em>expressed</em> and <em>storage</em>
sub-types.</li><li><a href=#instrumentation-and-constraint-ops>Instrumentation ops</a>
for assigning
instrumentation points within the computation where runtime statistics
may help guide the quantization process.</li></ul></li><li><p><a href=#integration-with-simulated-quantization-at-training-time>Integration with simulated quantization at training time</a></p></li><li><p><a href=#tflite-native-quantization>TFLite native quantization</a></p><ul><li>The TFLite op-set natively supports uniform-quantized variants.</li><li>Passes and tools exist to convert directly from the <em>TensorFlow</em> dialect
to the TFLite quantized operation set.</li></ul></li><li><p><a href=#fxpmath-dialect><em>FxpMath</em> dialect</a>
containing (experimental) generalized
representations of fixed-point math operations and conversions:</p><ul><li><a href=#real-math-ops>Real math ops</a>
representing common combinations of
arithmetic operations that closely match corresponding fixed-point math
concepts (as opposed to being spread across multiple ops as is typical
in source dialects).</li><li><a href=#fixed-point-math-ops>Fixed-point math ops</a>
that for carrying out
computations on integers, as are typically needed by uniform
quantization schemes.</li><li>Passes to lower from real math operations to fixed-point math operations.</li></ul></li><li><p><a href=#solver-tools>Solver tools</a>
which can (experimentally and generically
operate on computations expressed in the <em>FxpMath</em> dialect in order to
convert from floating point types to appropriate <em>QuantizedTypes</em>, allowing
the computation to be further lowered to integral math operations.</p></li></ul><p>Not every application of quantization will use all of these facilities. Specifically, the
TensorFlow to TensorFlow Lite conversion uses the QuantizedTypes but has its own
operations for type conversion and expression of the supporting math.</p><h2 id=quantization-dialect>Quantization Dialect</h2><h3 id=quantized-type>Quantized type</h3><p>TODO : Flesh this section out.</p><ul><li>QuantizedType base class</li><li>UniformQuantizedType</li></ul><h3 id=quantized-type-conversion-operations>Quantized type conversion operations</h3><ul><li>qcast : Convert from an expressed type to QuantizedType</li><li>dcast : Convert from a QuantizedType to its expressed type</li><li>scast : Convert between a QuantizedType and its storage type</li></ul><h3 id=instrumentation-and-constraint-operations>Instrumentation and constraint operations</h3><ul><li>const_fake_quant : Emulates the logic of the historic TensorFlow
fake_quant_with_min_max_args operation.</li><li>stats_ref : Declares that statistics should be gathered at this point with a
unique key and made available to future passes of the solver.</li><li>stats : Declares inline statistics (per layer and per axis) for the point in
the computation. stats_ref ops are generally converted to statistical operations once
trial runs have been performed.</li><li>coupled_ref : Declares points in the computation to be coupled from a type
inference perspective based on a unique key.</li></ul><h2 id=integration-with-simulated-quantization-at-training-time>Integration with simulated quantization at training time</h2><p>TensorFlow has historically used the
<a href=https://www.tensorflow.org/api_docs/python/tf/quantization/fake_quant_with_min_max_args>tf.quantization.fake_quant_*</a>
family of operations to simulate the effect of quantization at training time.</p><p>As originally implemented, TensorFlow Lite was the primary user of such
operations at inference time. When quantized inference was enabled, if every
eligible tensor passed through an appropriate fake_quant node (the rules of
which tensors can have fake_quant applied are somewhat involved), then
TensorFlow Lite would use the attributes of the fake_quant operations to make a
judgment about how to convert to use kernels from its quantized operations subset.</p><p>In MLIR-based quantization, fake_quant_* operationss are handled by converting them to
a sequence of *qcast* (quantize) followed by *dcast* (dequantize) with an
appropriate *UniformQuantizedType* as the target of the qcast operation.</p><p>This allows subsequent compiler passes to preserve the knowledge that
quantization was simulated in a certain way, while giving the compiler
flexibility to move the casts as it simplifies the computation and converts it
to a form based on integral arithmetic.</p><p>This scheme also naturally allows computations that are <em>partially quantized</em>
where the parts which could not be reduced to integral operationss are still carried out
in floating point with appropriate conversions at the boundaries.</p><h2 id=tflite-native-quantization>TFLite native quantization</h2><p>TODO : Flesh this out</p><h3 id=general-algorithm>General algorithm</h3><ol><li>Take input min/max information and set the ArrayInfo (which really is
InputOrOutputArrayInfo.</li><li>In LegalizeTF, convert ArrayInfo min/max to tf.Quantize and tf.Dequantize
nodes. (or tf.FakeQuant) Convert all constant FakeQuants to (tf.FQ -> tfl.Q
-> tfl.DQ).</li><li>Hardcode logic/propagation needs to happen here.</li><li>Run TF constant folding.</li><li>In PrepareTFL, convert all tf.FQ to (tfl.Q -> tfl.DQ).</li><li>Run quantization pass that take (tfl.DQ (for both input and weights) -> op
-> tfl.Q) and replaces with (op). Also replace (constant_float -> tfl.Q)
with (constant_quant).</li></ol><h2 id=fxpmath-dialect>FxpMath dialect</h2><h3 id=real-math-operations>Real math operations</h3><p>Note that these all support explicit clamps, which allows for simple fusions and
representation of some common sequences quantization-compatible math. Of
addition, some support explicit biases, which are often represented as separate
adds in source dialects.</p><p>TODO: This operation set is still evolving and needs to be completed.</p><ul><li>RealBinaryOp<ul><li>RealAddEwOp</li><li>RealSubEwOp</li><li>RealMulEwOp</li><li>RealDivEwOp</li></ul></li><li>RealUnaryOp<ul><li>IDENTITY</li><li>TANH</li><li>SIGMOID</li><li>EXP</li><li>LOG</li><li>NEG</li><li>RSQRT</li><li>SIN</li><li>SQUARE</li><li>SQRT</li><li>CMPZ</li><li>CMPNZ</li><li>CMPLZ</li><li>CMPGZ</li></ul></li></ul><h3 id=fixed-point-math-operationss>Fixed-point math operationss</h3><p>TODO: This operation set only has enough operations to lower a simple power-of-two
RealAddEwOp.</p><ul><li>RoundingDivideByPotFxpOp</li><li>SaturatingAddFxpOp</li></ul><h2 id=solver-tools>Solver tools</h2><p>Solver tools exist to analyze an MLIR-computation, expressed in either a
supported source dialect or in the <em>real math ops</em> set and solve for appropriate
QuantizedTypes that allow the computation to be lowered to integral math.</p><p>These tools are an active area of work and may be expanded in the future to
adjacent areas such as solving for transformations to other kinds of lower
precision types (i.e. bfloat16 or fp16).</p><p>Solver tools are expected to operate in several modes, depending on the
computation and the training characteristics of the model:</p><ul><li><p><em>Transform</em> : With all available information in the MLIR computation, infer
boundaries where the computation can be carried out with integral math and
change types accordingly to appropriate QuantizedTypes:</p><ul><li>For passthrough ops which do not perform active math, change them to
operate directly on the storage type, converting in and out at the edges
via scast operations.</li><li>For operations that have the <em>Quantizable</em> trait, the type can be set directly.
This includes operations from the [real math ops set]{#real-math-ops}.</li><li>For others, encase them in appropriate dcast/qcast operations, presuming that
some follow-on pass will know what to do with them.</li></ul></li><li><p><em>Instrument</em> : Most of the time, there are not sufficient implied
constraints within a computation to perform many transformations. For this
reason, the solver can insert instrumentation operations at points where additional
runtime statistics may yield solutions. It is expected that such
computations will be lowered as-is for execution, run over an appropriate
evaluation set, and statistics at each instrumentation point made available for a
future invocation of the solver.</p></li><li><p><em>Simplify</em> : A variety of passes and simplifications are applied once
QuantizedTypes are added in order to arrive at a computation that is
expressed in as much integral math, with the fewest number of casts as
possible.</p></li></ul><div class=edit-meta><br></div><nav class=pagination><a class="nav nav-prev" href=/docs/Passes/ title="MLIR Passes"><i class="fas fa-arrow-left" aria-hidden=true></i>Prev - MLIR Passes</a>
<a class="nav nav-next" href=/docs/Rationale/ title="MLIR Rationale">Next - MLIR Rationale <i class="fas fa-arrow-right" aria-hidden=true></i></a></nav><footer><p class=powered>Powered by <a href=https://gohugo.io>Hugo</a>. Theme by <a href=https://themes.gohugo.io/hugo-theme-techdoc/>TechDoc</a>. Designed by <a href=https://github.com/thingsym/hugo-theme-techdoc>Thingsym</a>.</p></footer></main><div class=sidebar><nav class=slide-menu><ul><li><a href=https://mlir.llvm.org/>Home</a></li><li><a href=/talks/>Talks and Related Publications</a></li><li><a href=/users/>Users of MLIR</a></li><li class=has-sub-menu><a href=/getting_started/>Getting Started<span class="mark closed">+</span></a><ul class=sub-menu><li><a href=/getting_started/Debugging/>Debugging</a></li><li><a href=/getting_started/Faq/>FAQ</a></li><li><a href=/getting_started/Contributing/>How to Contribute</a></li><li><a href=/getting_started/DeveloperGuide/>Developer Guide</a></li><li><a href=/getting_started/openprojects/>Open Projects</a></li><li><a href=/getting_started/Glossary/>Glossary</a></li><li><a href=/getting_started/TestingGuide/>Testing Guide</a></li></ul></li><li class="parent has-sub-menu"><a href=/docs/>Code Documentation<span class="mark opened">-</span></a><ul class=sub-menu><li class=has-sub-menu><a href=/docs/Dialects/>Dialects<span class="mark closed">+</span></a><ul class=sub-menu><li><a href=/docs/Dialects/Affine/>'affine' Dialect</a></li><li><a href=/docs/Dialects/FxpMathDialect/>'fxpmath' Dialect</a></li><li><a href=/docs/Dialects/GPU/>'gpu' Dialect</a></li><li><a href=/docs/Dialects/Linalg/>'linalg' Dialect</a></li><li><a href=/docs/Dialects/LLVM/>'llvm' Dialect</a></li><li><a href=/docs/Dialects/LoopDialect/>'loop' Dialect</a></li><li><a href=/docs/Dialects/NVVMDialect/>'nvvm' Dialect</a></li><li><a href=/docs/Dialects/OpenMPDialect/>'omp' Dialect</a></li><li><a href=/docs/Dialects/QuantDialect/>'quant' Dialect</a></li><li><a href=/docs/Dialects/ROCDLDialect/>'rocdl' Dialect</a></li><li><a href=/docs/Dialects/ShapeDialect/>'shape' Dialect</a></li><li><a href=/docs/Dialects/SPIR-V/>'spv' Dialect</a></li><li><a href=/docs/Dialects/Standard/>'std' Dialect</a></li><li><a href=/docs/Dialects/Vector/>'vector' Dialect</a></li></ul></li><li class=has-sub-menu><a href=/docs/Tutorials/Toy/>Toy<span class="mark closed">+</span></a><ul class=sub-menu><li><a href=/docs/Tutorials/Toy/Ch-1/>Chapter 1: Toy Tutorial Introduction</a></li><li><a href=/docs/Tutorials/Toy/Ch-2/>Chapter 2: Emitting Basic MLIR</a></li><li><a href=/docs/Tutorials/Toy/Ch-3/>Chapter 3: High-level Language-Specific Analysis and Transformation</a></li><li><a href=/docs/Tutorials/Toy/Ch-4/>Chapter 4: Enabling Generic Transformation with Interfaces</a></li><li><a href=/docs/Tutorials/Toy/Ch-5/>Chapter 5: Partial Lowering to Lower-Level Dialects for Optimization</a></li><li><a href=/docs/Tutorials/Toy/Ch-6/>Chapter 6: Lowering to LLVM and CodeGeneration</a></li><li><a href=/docs/Tutorials/Toy/Ch-7/>Chapter 7: Adding a Composite Type to Toy</a></li></ul></li><li><a href=/docs/EDSC/>Background: declarative builders API</a></li><li><a href=/docs/ConversionToLLVMDialect/>Conversion to the LLVM Dialect</a></li><li><a href=/docs/CreatingADialect/>Creating a Dialect</a></li><li><a href=/docs/DialectConversion/>Dialect Conversion</a></li><li><a href=/docs/Diagnostics/>Introduction and Usage Guide to MLIR's Diagnostics Infrastructure</a></li><li><a href=/docs/Interfaces/>Introduction to MLIR Interfaces</a></li><li><a href=/docs/Traits/>Introduction to MLIR Operation Traits</a></li><li><a href=/docs/RationaleLinalgDialect/>Linalg Dialect Rationale: The Case For Compiler-Friendly Custom Operations</a></li><li><a href=/docs/GenericDAGRewriter/>MLIR Generic DAG Rewriter Infrastructure</a></li><li><a href=/docs/Passes/>MLIR Passes</a></li><li class=active><a href=/docs/Quantization/>MLIR Quantization</a></li><li><a href=/docs/Rationale/>MLIR Rationale</a></li><li><a href=/docs/LangRef/>MLIR Specification</a></li><li><a href=/docs/MLIRForGraphAlgorithms/>MLIR: Incremental Application to Graph Algorithms in ML Frameworks</a></li><li><a href=/docs/RationaleSimplifiedPolyhedralForm/>MLIR: The case for a simplified polyhedral form</a></li><li><a href=/docs/Canonicalization/>Operation Canonicalization</a></li><li><a href=/docs/QuickstartRewrites/>Quickstart tutorial to adding MLIR graph rewrite</a></li><li><a href=/docs/DefiningAttributesAndTypes/>Quickstart tutorial to defining custom dialect attributes and types</a></li><li><a href=/docs/ShapeInference/>Shape inference</a></li><li><a href=/docs/SymbolsAndSymbolTables/>Symbols and Symbol Tables</a></li><li><a href=/docs/DeclarativeRewrites/>Table-driven Declarative Rewrite Rule (DRR)</a></li><li><a href=/docs/OpDefinitions/>Table-driven Operation Definition Specification (ODS)</a></li><li><a href=/docs/UsageOfConst/>Usage of 'Const' in MLIR, for core IR types</a></li><li><a href=/docs/WritingAPass/>Writing a Pass</a></li></ul></li></ul></nav><div class=sidebar-footer></div></div></div><a href=# id=backtothetop-fixed class=backtothetop data-backtothetop-duration=600 data-backtothetop-easing=easeOutQuart data-backtothetop-fixed-fadein=1000 data-backtothetop-fixed-fadeout=1000 data-backtothetop-fixed-bottom=10 data-backtothetop-fixed-right=20><span class="fa-layers fa-fw"><i class="fas fa-circle"></i><i class="fas fa-arrow-circle-up"></i></span></a></div></body></html>